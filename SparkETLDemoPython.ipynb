{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ETL Demo Python\n",
    "\n",
    "This demo written in Python for Watson Data Studio illustrates the use of a Spark cluster to perform ETL. It imports data in flat files into Spark DataFrames, manipulates the data, aggregates it  and then writes the result out to a relational  database. The advantage of using Spark for this is scalability  (by using a larger cluster one can achieve close to linear scalability) and simplified error recovery (a failed attempt at running this ETL job can be repeated at any stage and the final result will be the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 Read in the source data\n",
    "We read two CSV files. One has statistics about Social Security payments for the state of Texas by zipcode and the other maps US zipcodes to US counties so we can aggregate the Social Security data by county rather than zipcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the input data files from Github and stick them in in gpfs using wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wget if you don't already have it.\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_ssdata = 'https://raw.githubusercontent.com/djccarew/sparketldemo/master/data/oasdi-tx-clean.csv'\n",
    "link_to_zipdata = 'https://raw.githubusercontent.com/djccarew/sparketldemo/master/data/zip_codes_states.csv'\n",
    "social_security_data_file = wget.download(link_to_ssdata)\n",
    "\n",
    "print(social_security_data_file)\n",
    "\n",
    "zipcode_data_file = wget.download(link_to_zipdata)\n",
    "\n",
    "print(zipcode_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Social Security data file into a DataFrame using a schema. Note the schema can be inferred but the inferred schema typically converts various numeric types to string so it's better to specify the schema so you know what you end up with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ssdata_schema = StructType([\n",
    "    StructField(\"Zip\", StringType(), False),\n",
    "    StructField(\"NumTotal\", IntegerType(), False),\n",
    "    StructField(\"NumRetired\", IntegerType(), False),\n",
    "    StructField(\"NumDisabled\", IntegerType(), False),\n",
    "    StructField(\"NumWidowerOrParent\", IntegerType(), False),\n",
    "    StructField(\"NumSpouses\", IntegerType(), False),\n",
    "    StructField(\"NumChildren\", IntegerType(), False),\n",
    "    StructField(\"BenTotal\", IntegerType(), False),\n",
    "    StructField(\"BenRetired\", IntegerType(), False),\n",
    "    StructField(\"BenWidowerOrParent\", IntegerType(), False),\n",
    "    StructField(\"NumSeniors\", IntegerType(), False)])\n",
    "\n",
    "df_ssdata_raw = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(social_security_data_file, schema=ssdata_schema)\n",
    "    \n",
    "df_ssdata_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for zipcode data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata_schema = StructType([\n",
    "    StructField(\"Zip\", StringType(), False),\n",
    "    StructField(\"Latitude\", DoubleType(), False),\n",
    "    StructField(\"Longitude\", DoubleType(), False),\n",
    "    StructField(\"City\", StringType(), False),\n",
    "    StructField(\"State\", StringType(), False),\n",
    "    StructField(\"County\", StringType(), False)])\n",
    "    \n",
    "\n",
    "df_zipdata_raw = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .load(zipcode_data_file, schema=zipdata_schema)\n",
    "df_zipdata_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Transform raw source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only need County name and zip code columns for this demo so we don't use the other columns in the zipcode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties = df_zipdata_raw.select('Zip','County')\n",
    "df_counties.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join Social Security data with zipcode data to add a County column to Social Security data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssdata_counties = df_ssdata_raw.join(df_counties, \"Zip\")\n",
    "df_ssdata_counties.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need the zipcode column anymore since we'll be aggregating by County instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssdata_counties = df_ssdata_counties.drop(\"Zip\")\n",
    "df_ssdata_counties.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temp view so we can do the \"by county\" aggregation via SQL rather than using the Spark SQL DataFrame API. (Doing it via SQL is usually easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssdata_counties.createOrReplaceTempView(\"aggregated_by_county\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL query to aggregate Social Security data by county and sort by county name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssdata_data_by_county = spark.sql(\"select County, sum(NumTotal) as NumTotal, sum(NumRetired) as NumRetired, sum(NumDisabled) as NumDisabled, sum(NumWidowerOrParent) as NumWidowerOrParent, sum(NumSpouses) as NumSpouses, sum(NumChildren) as NumChildren, sum(BenTotal) as BenTotal, sum(BenRetired) as BenRetired, sum(BenWidowerOrParent) as BenWidowerOrParent, sum(NumSeniors) as NumSeniors from aggregated_by_county group by County order by County\")\n",
    "df_ssdata_data_by_county.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Write modified data to target database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the jdbc method of the DataFrameWriter to write the modified data to the target db. Appropriate credentials for the target db need to be set up first. Modify the code below with the approparaite values for your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = 'your-jdbc-url'\n",
    "dest_table = 'your-table-name'\n",
    "jdbc_properties = {\n",
    "    'driver': 'com.ibm.db2.jcc.DB2Driver',\n",
    "    'user': 'your-db-user',\n",
    "    'password': 'your-db-password'\n",
    "}\n",
    "\n",
    "df_ssdata_data_by_county.write.jdbc(jdbc_url, table=dest_table, mode='overwrite', properties=jdbc_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
