{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ETL Demo Scala\n",
    "\n",
    "This demo written in Scala for Watson Data Studio illustrates the use of a Spark cluster to perform ETL. It imports data in flat files into Spark DataFrames, manipulates the data, aggregates it  and then writes the result out to a relational  database. The advantage of using Spark for this is scalability  (by using a larger cluster one can achieve close to linear scalability) and simplified error recovery (a failed attempt at running this ETL job can be repeated at any stage and the final result will be the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Read in the source data\n",
    "We read  two CSV files. One has statistics about Social Security payments for the state of Texas by zipcode and the other maps US zipcodes to US counties so we can aggregate the Social Security data by county rather than zipcode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the input data files from Github and stick them in in gpfs using wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.sys.process._\n",
    "\n",
    "val socialSecurityDataFile = \"oasdi-tx-clean.csv\"\n",
    "val zipcodeDataFile = \"zip_codes_states.csv\"\n",
    "\n",
    "s\"wget -O $socialSecurityDataFile https://raw.githubusercontent.com/djccarew/sparketldemo/master/data/oasdi-tx-clean.csv\".!\n",
    "s\"wget -O $zipcodeDataFile https://raw.githubusercontent.com/djccarew/sparketldemo/master/data/zip_codes_states.csv\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Social Security data file into a DataFrame using a schema. Note the schema can be inferred but the inferred schema typically converts various numeric types to string so it's better to specify the schema so you know what you end up with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder().\n",
    "    getOrCreate()\n",
    "\n",
    "// Specify schema for resulting DataFrame\n",
    "val socialSecurityDataSchema = StructType(Array(\n",
    "        StructField(\"Zip\", StringType, false),\n",
    "        StructField(\"NumTotal\", IntegerType, false),\n",
    "        StructField(\"NumRetired\", IntegerType, false),\n",
    "        StructField(\"NumDisabled\", IntegerType, false),\n",
    "        StructField(\"NumWidowerOrParent\", IntegerType,false),\n",
    "        StructField(\"NumSpouses\", IntegerType, false),\n",
    "        StructField(\"NumChildren\", IntegerType, false),\n",
    "        StructField(\"BenTotal\", IntegerType, false),\n",
    "        StructField(\"BenRetired\", IntegerType, false),\n",
    "        StructField(\"BenWidowerOrParent\", IntegerType, false),\n",
    "        StructField(\"NumSeniors\", IntegerType, false)))\n",
    "\n",
    "// Read CSV file into DataFrame using schema \n",
    "val dfSocialSecurityDataRaw = spark.\n",
    "    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(socialSecurityDataSchema).\n",
    "    load(\"oasdi-tx-clean.csv\")\n",
    "\n",
    "// Validate DataFrame was created correctly\n",
    "dfSocialSecurityDataRaw.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for zipcode data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val zipDataSchema = StructType(Array(\n",
    "      StructField(\"Zip\", StringType, false),\n",
    "      StructField(\"Latitude\", DoubleType, false),\n",
    "      StructField(\"Longitude\", DoubleType, false),\n",
    "      StructField(\"City\", StringType, false),\n",
    "      StructField(\"State\", StringType, false),\n",
    "      StructField(\"County\", StringType, false)))\n",
    "\n",
    "val dfZipDataRaw = spark.\n",
    "    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(zipDataSchema).\n",
    "    load(\"zip_codes_states.csv\")\n",
    "\n",
    "dfZipDataRaw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Transform raw source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only need County name and zip code columns for this demo so we don't use the other columns in the zipcode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCounties = dfZipDataRaw.select(\"Zip\", \"County\")\n",
    "dfCounties.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join Social Security data with zipcode data to add a County column to Social Security data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var dfSocialSecurityDataWithCounty = dfSocialSecurityDataRaw.join(dfCounties,\"Zip\")\n",
    "dfSocialSecurityDataWithCounty.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need the zipcode column anymore since we'll be aggregating by County instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSocialSecurityDataWithCounty = dfSocialSecurityDataWithCounty.drop(\"Zip\")\n",
    "dfSocialSecurityDataWithCounty.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temp view so we can do the \"by county\" aggregation via SQL rather than using the Spark SQL DataFrame API. (Doing it via SQL is usually easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSocialSecurityDataWithCounty.createOrReplaceTempView(\"aggregated_by_county\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL query to aggregate Social Security data by county and sort by county name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfSocialSecurityDataByCounty = spark.sql(\"select County, sum(NumTotal) as NumTotal, sum(NumRetired) as NumRetired, sum(NumDisabled) as NumDisabled, sum(NumWidowerOrParent) as NumWidowerOrParent, sum(NumSpouses) as NumSpouses, sum(NumChildren) as NumChildren, sum(BenTotal) as BenTotal, sum(BenRetired) as BenRetired, sum(BenWidowerOrParent) as BenWidowerOrParent, sum(NumSeniors) as NumSeniors from aggregated_by_county group by County order by County\")\n",
    "dfSocialSecurityDataByCounty.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Write modified data to target database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the jdbc method of the  DataFrameWriter to write the  modified data to the target db. Appropriate credentials for the target db need to be set up first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val jdbcURL = \"jdbc:db2://dashdb-txn-sbox-yp-dal09-04.services.dal.bluemix.net:50000/BLUDB\"\n",
    "val destTable = \"WTM57848.TXSSBYCOUNTY\"\n",
    "\n",
    "val jdbcProperties = new java.util.Properties\n",
    "jdbcProperties.setProperty(\"driver\", \"com.ibm.db2.jcc.DB2Driver\")\n",
    "jdbcProperties.setProperty(\"user\",\"wtm57848\")\n",
    "jdbcProperties.setProperty(\"password\",\"2ncndbg7bpb7^4p6\")\n",
    "\n",
    "dfSocialSecurityDataByCounty.write.mode(\"overwrite\").jdbc(jdbcURL, destTable, jdbcProperties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
